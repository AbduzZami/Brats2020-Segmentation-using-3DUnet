{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":6418531,"sourceType":"datasetVersion","datasetId":3702271},{"sourceId":7356752,"sourceType":"datasetVersion","datasetId":4272868}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -U nilearn\n! pip install segmentation-models-3D\n! pip install classification-models-3D\n! pip install visualkeras\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\nimport os\n#General Libraries\n\nimport shutil\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n#Model libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nimport visualkeras\n# from keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.utils import plot_model\nimport segmentation_models_3D as sm\nfrom keras import callbacks\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\n\nTRAIN_DATASET_PATH='/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-28T14:26:18.220338Z","iopub.execute_input":"2024-04-28T14:26:18.220649Z","iopub.status.idle":"2024-04-28T14:28:46.887794Z","shell.execute_reply.started":"2024-04-28T14:26:18.220622Z","shell.execute_reply":"2024-04-28T14:28:46.886797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import numpy as np\n# from skimage.transform import resize\n# from skimage import exposure\n\n# def resize_3d_image(image, new_shape):\n#     resized_image = np.zeros(new_shape)\n#     for i in range(image.shape[-1]):\n#         resized_image[..., i] = resize(image[..., i], new_shape[:-1], mode='constant', anti_aliasing=True)\n#     return resized_image\n\n# def histogram_equalization_3d(image):\n#     equalized_image = np.zeros_like(image)\n#     for i in range(image.shape[-1]):\n#         equalized_image[..., i] = exposure.equalize_hist(image[..., i], nbins=128)\n#     return equalized_image\n\n# def process_images_in_folder(folder_path, output_folder_path):\n#     # Ensure output folder exists\n#     os.makedirs(output_folder_path, exist_ok=True)\n\n#     # List all files in the input folder\n#     files = os.listdir(folder_path)\n\n#     for file in files:\n#         # Assuming the images are in a specific format (e.g., '.npy'). Modify accordingly.\n#         if file.endswith('.npy'):\n#             # Load the image\n#             original_image = np.load(os.path.join(folder_path, file))\n\n#             # Resize the image\n#             new_shape = (25, 25, 25, 3)\n#             resized_image = resize_3d_image(original_image, new_shape)\n\n#             # Histogram equalization\n#             equalized_image = histogram_equalization_3d(resized_image)\n\n#             # Save the processed image to the output folder\n#             output_file_path = os.path.join(output_folder_path, file)\n#             np.save(output_file_path, equalized_image)\n\n# if __name__ == \"__main__\":\n#     # Specify the input and output folders\n#     input_folder_path = \"/path/to/input_folder\"\n#     output_folder_path = \"/path/to/output_folder\"\n\n#     # Process images in the input folder and save to the output folder\n#     process_images_in_folder(input_folder_path, output_folder_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:46.889774Z","iopub.execute_input":"2024-04-28T14:28:46.890920Z","iopub.status.idle":"2024-04-28T14:28:46.896688Z","shell.execute_reply.started":"2024-04-28T14:28:46.890881Z","shell.execute_reply":"2024-04-28T14:28:46.895584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the list of all folders. Exclude 'BraTS20_Training_355'. As its segmenation image has some weired name.\nData_dir = [f.path for f in os.scandir('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData') if f.is_dir()]\ndef Name(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\nn=Name(Data_dir)\nN=[t for t in n if t!='BraTS20_Training_355']#This file has weird seg name","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:46.897903Z","iopub.execute_input":"2024-04-28T14:28:46.898227Z","iopub.status.idle":"2024-04-28T14:28:46.971958Z","shell.execute_reply.started":"2024-04-28T14:28:46.898193Z","shell.execute_reply":"2024-04-28T14:28:46.971207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Image_explorer:\n    def __init__(self):\n        np.random.seed(4)\n        i=np.random.randint(0,len(N)-1)\n        path=os.path.join(TRAIN_DATASET_PATH,N[i])\n        p=os.listdir(path)\n        t1 = [i for i, s in enumerate(p) if 't1.nii' in s]\n        t2 = [i for i, s in enumerate(p) if 't2.nii' in s]\n        t1ce = [i for i, s in enumerate(p) if 't1ce.nii' in s]\n        seg = [i for i, s in enumerate(p) if 'seg.nii' in s]\n        flair = [i for i, s in enumerate(p) if 'flair.nii' in s]\n        self.test_image_flair=nib.load(os.path.join(path,p[flair[0]])).get_fdata()\n        self.test_image_t1=nib.load(os.path.join(path,p[t1[0]])).get_fdata()\n        self.test_image_t1ce=nib.load(os.path.join(path,p[t1ce[0]])).get_fdata()\n        self.test_image_t2=nib.load(os.path.join(path,p[t2[0]])).get_fdata()\n        self.test_seg=nib.load(os.path.join(path,p[seg[0]])).get_fdata()\n\n    def Axial_View(self,layer):\n        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 5))\n        ax1.imshow(self.test_image_flair[:,:,layer], cmap = 'gray')\n        ax1.set_title('Flair')\n        ax2.imshow(self.test_image_t1[:,:,layer], cmap = 'gray')\n        ax2.set_title('T1')\n        ax3.imshow(self.test_image_t1ce[:,:,layer], cmap = 'gray')\n        ax3.set_title('T1ce')\n        ax4.imshow(self.test_image_t2[:,:,layer], cmap = 'gray')\n        ax4.set_title('T2')\n        ax5.imshow(self.test_seg[:,:,layer])\n        ax5.set_title('Segmented')\n        fig.suptitle('Axial View',fontsize=30)\n        fig.tight_layout()\n        plt.show()\n\n    def Sagittal_View(self,layer):\n        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 5))\n        ax1.imshow(np.rot90(self.test_image_flair[layer,:,:]), cmap = 'gray')\n        ax1.set_title('Flair')\n        ax2.imshow(np.rot90(self.test_image_t1[layer,:,:]), cmap = 'gray')\n        ax2.set_title('T1')\n        ax3.imshow(np.rot90(self.test_image_t1ce[layer,:,:]), cmap = 'gray')\n        ax3.set_title('T1ce')\n        ax4.imshow(np.rot90(self.test_image_t2[layer,:,:]), cmap = 'gray')\n        ax4.set_title('T2')\n        ax5.imshow(np.rot90(self.test_seg[layer,:,:]))\n        ax5.set_title('Segmented')\n        fig.suptitle('Sagittal View',fontsize=30)\n        fig.tight_layout()\n        plt.show()\n\n    def Coronal_View(self,layer):\n        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 5))\n        ax1.imshow(np.rot90(self.test_image_flair[:,layer,:]), cmap = 'gray')\n        ax1.set_title('Flair')\n        ax2.imshow(np.rot90(self.test_image_t1[:,layer,:]), cmap = 'gray')\n        ax2.set_title('T1')\n        ax3.imshow(np.rot90(self.test_image_t1ce[:,layer,:]), cmap = 'gray')\n        ax3.set_title('T1ce')\n        ax4.imshow(np.rot90(self.test_image_t2[:,layer,:]), cmap = 'gray')\n        ax4.set_title('T2')\n        ax5.imshow(np.rot90(self.test_seg[:,layer,:]))\n        ax5.set_title('Segmented')\n        fig.suptitle('Coronal View',fontsize=30)\n        fig.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:46.974025Z","iopub.execute_input":"2024-04-28T14:28:46.974313Z","iopub.status.idle":"2024-04-28T14:28:46.995214Z","shell.execute_reply.started":"2024-04-28T14:28:46.974276Z","shell.execute_reply":"2024-04-28T14:28:46.994329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image_flair=nib.load('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/BraTS20_Training_355_flair.nii').get_fdata()\nImage=Image_explorer\nImage().Axial_View(layer=75);\nImage().Sagittal_View(layer=75)\nImage().Coronal_View(layer=150)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:46.996337Z","iopub.execute_input":"2024-04-28T14:28:46.996637Z","iopub.status.idle":"2024-04-28T14:28:52.259313Z","shell.execute_reply.started":"2024-04-28T14:28:46.996613Z","shell.execute_reply":"2024-04-28T14:28:52.258374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the main folder path\nmain_folder = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n\n# Get a list of all items (files and folders) within the main folder\nall_items = os.listdir(main_folder)\n\n# Filter out only the folders\nsubfolders = [item for item in all_items if os.path.isdir(os.path.join(main_folder, item))]\n\n# Print the list of subfolders\nprint(\"Subfolders:\")\nfor subfolder in subfolders[:5]:\n    # List .nii files in the folder\n    nii_files = [file for file in os.listdir('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'+subfolder) if file.endswith('.nii')]\n\n    # Iterate over the .nii files\n    for filename in nii_files:\n        file_path = os.path.join('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'+subfolder, filename)\n        img = nib.load(file_path)\n        data = img.get_fdata()\n\n        # Print the shape of the loaded data\n        print(f\"Shape of {filename}: {data.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:52.260788Z","iopub.execute_input":"2024-04-28T14:28:52.261140Z","iopub.status.idle":"2024-04-28T14:28:58.551308Z","shell.execute_reply.started":"2024-04-28T14:28:52.261108Z","shell.execute_reply":"2024-04-28T14:28:58.550360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import nibabel as nib\n# import numpy as np\n\n# # Define the main folder path\n# main_folder = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n\n# # Define the output folder path\n# output_folder = '/kaggle/working/processed_images/training'\n\n# # Create the output folder if it doesn't exist\n# os.makedirs(output_folder, exist_ok=True)\n\n# # Define a function for histogram equalization\n# def histogram_equalization_3d(data):\n#     # Your implementation of histogram equalization for 3D data\n#     # This function is assumed to perform histogram equalization on each slice of the 3D volume\n#     return data  # Placeholder, replace with actual implementation\n\n# # Get a list of all items (files and folders) within the main folder\n# all_items = os.listdir(main_folder)\n\n# # Filter out only the folders\n# subfolders = [item for item in all_items if os.path.isdir(os.path.join(main_folder, item))]\n\n# # Print the list of subfolders\n# print(\"Subfolders:\")\n# for subfolder in subfolders[:5]:\n#     # List .nii files in the folder\n#     nii_files = [file for file in os.listdir(os.path.join(main_folder, subfolder)) if file.endswith('.nii')]\n\n#     # Iterate over the .nii files\n#     for filename in nii_files:\n#         file_path = os.path.join(main_folder, subfolder, filename)\n#         img = nib.load(file_path)\n#         data = img.get_fdata()\n        \n#         # Apply histogram equalization\n#         image = histogram_equalization_3d(data)\n\n#         # Print the shape of the loaded data\n#         print(f\"Shape of {filename}: {image.shape}\")\n\n#         # Create the directory for the current subfolder if it doesn't exist\n#         output_subfolder = os.path.join(output_folder, subfolder)\n#         os.makedirs(output_subfolder, exist_ok=True)\n\n#         # Save the processed image to the output folder\n#         output_file_path = os.path.join(output_subfolder, filename)  # Modify the filename if needed\n#         nib.save(nib.Nifti1Image(image, img.affine), output_file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:58.552799Z","iopub.execute_input":"2024-04-28T14:28:58.553157Z","iopub.status.idle":"2024-04-28T14:28:58.559048Z","shell.execute_reply.started":"2024-04-28T14:28:58.553124Z","shell.execute_reply":"2024-04-28T14:28:58.558157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# import os\n\n# # Define the directory path to be deleted\n# directory_to_delete = '/kaggle/working/processed_images'\n\n# # Check if the directory exists\n# if os.path.exists(directory_to_delete):\n#     # Delete the directory and its contents\n#     try:\n#         shutil.rmtree(directory_to_delete)\n#         print(f\"Directory '{directory_to_delete}' has been successfully deleted.\")\n#     except OSError as e:\n#         print(f\"Error: {directory_to_delete} : {e.strerror}\")\n# else:\n#     print(f\"Directory '{directory_to_delete}' does not exist.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:58.560309Z","iopub.execute_input":"2024-04-28T14:28:58.560882Z","iopub.status.idle":"2024-04-28T14:28:58.572808Z","shell.execute_reply.started":"2024-04-28T14:28:58.560850Z","shell.execute_reply":"2024-04-28T14:28:58.572006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom skimage.transform import resize\n\ndef resize_3d_image(image, new_shape):\n    resized_image = np.zeros(new_shape)\n    for i in range(image.shape[-1]):\n        resized_image[..., i] = resize(image[..., i], new_shape[:-1], mode='constant', anti_aliasing=True)\n    return resized_image\n\n# Example usage\noriginal_image = np.random.rand(50, 50, 50, 3)  # Example 3D image with shape (50, 50, 50, 3)\nnew_shape = (25, 25, 25, 3)  # New shape for the resized image\n\nresized_image = resize_3d_image(original_image, new_shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:58.573931Z","iopub.execute_input":"2024-04-28T14:28:58.574236Z","iopub.status.idle":"2024-04-28T14:28:58.719329Z","shell.execute_reply.started":"2024-04-28T14:28:58.574211Z","shell.execute_reply":"2024-04-28T14:28:58.718617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom skimage import exposure\n\ndef histogram_equalization_3d(image):\n    equalized_image = np.zeros_like(image)\n\n    for i in range(image.shape[-1]):\n        equalized_image[..., i] = exposure.equalize_hist(image[..., i], nbins=128)\n\n    return equalized_image\n\n# Example usage\noriginal_image = np.random.rand(128, 128, 128, 3)  # Example 3D image with shape (128, 128, 128, 3)\nequalized_image = histogram_equalization_3d(original_image)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:58.722605Z","iopub.execute_input":"2024-04-28T14:28:58.722858Z","iopub.status.idle":"2024-04-28T14:28:59.310447Z","shell.execute_reply.started":"2024-04-28T14:28:58.722836Z","shell.execute_reply":"2024-04-28T14:28:59.309610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    def __init__(self, list_IDs, batch_size=1, dim=(128,128,128),shuffle=False,channels=3,num_class=4):\n        'Initialization'\n        self.dim = dim\n        self.n_channels=channels\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.shuffle = shuffle\n        self.num_class=num_class\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Generate data\n        X=np.zeros((self.batch_size,*(self.dim),self.n_channels))\n        Y=np.zeros((self.batch_size,*(self.dim),self.num_class))\n        for i, ID in enumerate(list_IDs_temp):\n            case_path = os.path.join(TRAIN_DATASET_PATH, ID)\n            data_path = os.path.join(case_path, f'{ID}_t1.nii')\n            t1 = nib.load(data_path).get_fdata()\n            st1=MinMaxScaler()\n            t1=st1.fit_transform(t1.reshape(-1,t1.shape[-1])).reshape(t1.shape)\n            data_path = os.path.join(case_path, f'{ID}_flair.nii')\n            flair = nib.load(data_path).get_fdata()\n            stflair=MinMaxScaler()\n            flair=stflair.fit_transform(flair.reshape(-1,flair.shape[-1])).reshape(flair.shape)\n            data_path = os.path.join(case_path, f'{ID}_t1ce.nii')\n            t1ce = nib.load(data_path).get_fdata()\n            st1ce=MinMaxScaler()\n            t1ce=st1ce.fit_transform(t1ce.reshape(-1,t1ce.shape[-1])).reshape(t1ce.shape)\n            data_path = os.path.join(case_path, f'{ID}_seg.nii')\n            seg = nib.load(data_path)\n            x=np.stack([t1,flair,t1ce],axis=3)\n            seg=np.array(seg.get_fdata())\n            seg[seg==4]=3\n            seg=keras.utils.to_categorical(seg,self.num_class)\n#             X[i] = resize_3d_image(x, (128,128,128,3))\n#             X[i] = histogram_equalization_3d(X[i])\n#             Y[i] = resize_3d_image(seg, (128,128,128,4))\n            X[i]=x[56:184,56:184,13:141]#this slicing is important as GPU will run out of memory if we take the complete image\n            Y[i]=seg[56:184,56:184,13:141]\n\n        return X, Y","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:59.311720Z","iopub.execute_input":"2024-04-28T14:28:59.312025Z","iopub.status.idle":"2024-04-28T14:28:59.329665Z","shell.execute_reply.started":"2024-04-28T14:28:59.311998Z","shell.execute_reply":"2024-04-28T14:28:59.328667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.utils import Sequence\n# from sklearn.preprocessing import MinMaxScaler\n# import numpy as np\n# import nibabel as nib\n# import os\n\n# class DataGenerator(Sequence):\n#     def __init__(self, list_IDs, batch_size=1, dim=(128, 128, 128), shuffle=False, channels=3, num_class=4):\n#         'Initialization'\n#         self.dim = dim\n#         self.n_channels = channels\n#         self.batch_size = batch_size\n#         self.list_IDs = list_IDs\n#         self.shuffle = shuffle\n#         self.num_class = num_class\n#         self.on_epoch_end()\n\n#     def __len__(self):\n#         'Denotes the number of batches per epoch'\n#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n#     def __getitem__(self, index):\n#         'Generate one batch of data'\n#         # Generate indexes of the batch\n#         indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n#         # Find list of IDs\n#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n#         # Generate data\n#         X, y = self.__data_generation(list_IDs_temp)\n#         return X, y\n\n#     def on_epoch_end(self):\n#         self.indexes = np.arange(len(self.list_IDs))\n#         if self.shuffle == True:\n#             np.random.shuffle(self.indexes)\n\n#     def __data_generation(self, list_IDs_temp):\n#         'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n#         # Generate data\n#         X = np.zeros((self.batch_size, *self.dim, self.n_channels))\n#         Y = np.zeros((self.batch_size, *self.dim, self.num_class))\n\n#         for i, ID in enumerate(list_IDs_temp):\n#             case_path = os.path.join(TRAIN_DATASET_PATH, ID)\n\n#             # Load the image and get its original size\n#             data_path_t1 = os.path.join(case_path, f'{ID}_t1.nii')\n#             t1 = nib.load(data_path_t1).get_fdata()\n#             original_size = t1.shape\n\n#             # Normalize and stack channels\n#             st1 = MinMaxScaler()\n#             t1 = st1.fit_transform(t1.reshape(-1, t1.shape[-1])).reshape(t1.shape)\n\n#             data_path_flair = os.path.join(case_path, f'{ID}_flair.nii')\n#             flair = nib.load(data_path_flair).get_fdata()\n#             stflair = MinMaxScaler()\n#             flair = stflair.fit_transform(flair.reshape(-1, flair.shape[-1])).reshape(flair.shape)\n\n#             data_path_t1ce = os.path.join(case_path, f'{ID}_t1ce.nii')\n#             t1ce = nib.load(data_path_t1ce).get_fdata()\n#             st1ce = MinMaxScaler()\n#             t1ce = st1ce.fit_transform(t1ce.reshape(-1, t1ce.shape[-1])).reshape(t1ce.shape)\n\n#             x = np.stack([t1, flair, t1ce], axis=3)\n\n#             # Resize X based on the fixed size\n#             x_resized = tf.image.resize(x, size=self.dim, method=tf.image.ResizeMethod.BILINEAR)\n\n#             seg_path = os.path.join(case_path, f'{ID}_seg.nii')\n#             seg = nib.load(seg_path)\n#             seg = np.array(seg.get_fdata())\n#             seg[seg == 4] = 3\n#             seg = keras.utils.to_categorical(seg, self.num_class)\n\n#             # Resize Y based on the fixed size\n#             seg_resized = tf.image.resize(seg, size=self.dim, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n#             # Assign resized data to the batch\n#             X[i] = x_resized.numpy()\n#             Y[i] = seg_resized.numpy()\n\n#         return X, Y\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:59.330919Z","iopub.execute_input":"2024-04-28T14:28:59.331199Z","iopub.status.idle":"2024-04-28T14:28:59.344547Z","shell.execute_reply.started":"2024-04-28T14:28:59.331171Z","shell.execute_reply":"2024-04-28T14:28:59.343734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_ids = Name(Data_dir);\nTrain_ids=[t for t in Train_ids if t!='BraTS20_Training_355']\ntrain,test=train_test_split(Train_ids,test_size=(0.2),random_state=42)\nTrain,Validation=train_test_split(train,test_size=(0.2),random_state=42)\nTrain_datagen=DataGenerator(Train,batch_size=1)\nVal_datagen=DataGenerator(Validation,batch_size=1)\nTest_datagen=DataGenerator(test,batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:59.345539Z","iopub.execute_input":"2024-04-28T14:28:59.345814Z","iopub.status.idle":"2024-04-28T14:28:59.359127Z","shell.execute_reply.started":"2024-04-28T14:28:59.345791Z","shell.execute_reply":"2024-04-28T14:28:59.358266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(Train))\nprint(len(Validation))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:59.360246Z","iopub.execute_input":"2024-04-28T14:28:59.360776Z","iopub.status.idle":"2024-04-28T14:28:59.368199Z","shell.execute_reply.started":"2024-04-28T14:28:59.360751Z","shell.execute_reply":"2024-04-28T14:28:59.367323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y=Train_datagen.__getitem__(50)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:28:59.371073Z","iopub.execute_input":"2024-04-28T14:28:59.371407Z","iopub.status.idle":"2024-04-28T14:29:01.322124Z","shell.execute_reply.started":"2024-04-28T14:28:59.371382Z","shell.execute_reply":"2024-04-28T14:29:01.321124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:01.323390Z","iopub.execute_input":"2024-04-28T14:29:01.323712Z","iopub.status.idle":"2024-04-28T14:29:01.329013Z","shell.execute_reply.started":"2024-04-28T14:29:01.323686Z","shell.execute_reply":"2024-04-28T14:29:01.327891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = 0  # checking the 2nd sample of the first batch\nyhat = y[sample]\nyhat = np.argmax(yhat, axis=-1)\n\nlayer = 80\n\nfig, axes = plt.subplots(3, 4, figsize=(15, 10))\n\n# Axial View\naxes[0, 0].imshow(np.rot90(X[sample, layer, :, :, 0]), cmap='gray')\naxes[0, 0].set_title('Axial - Channel 1')\naxes[0, 0].axis('off')\n\naxes[0, 1].imshow(np.rot90(X[sample, layer, :, :, 1]), cmap='gray')\naxes[0, 1].set_title('Axial - Channel 2')\naxes[0, 1].axis('off')\n\naxes[0, 2].imshow(np.rot90(X[sample, layer, :, :, 2]), cmap='gray')\naxes[0, 2].set_title('Axial - Channel 3')\naxes[0, 2].axis('off')\n\naxes[0, 3].imshow(np.rot90(yhat[layer, :, :]))\naxes[0, 3].set_title('Axial - Segmentation Mask')\naxes[0, 3].axis('off')\n\n# Sagittal View\naxes[1, 0].imshow(np.rot90(X[sample, :, layer, :, 0]), cmap='gray')\naxes[1, 0].set_title('Sagittal - Channel 1')\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(np.rot90(X[sample, :, layer, :, 1]), cmap='gray')\naxes[1, 1].set_title('Sagittal - Channel 2')\naxes[1, 1].axis('off')\n\naxes[1, 2].imshow(np.rot90(X[sample, :, layer, :, 2]), cmap='gray')\naxes[1, 2].set_title('Sagittal - Channel 3')\naxes[1, 2].axis('off')\n\naxes[1, 3].imshow(np.rot90(yhat[:, layer, :]))\naxes[1, 3].set_title('Sagittal - Segmentation Mask')\naxes[1, 3].axis('off')\n\n# Coronal View\naxes[2, 0].imshow(np.rot90(X[sample, :, :, layer, 0]), cmap='gray')\naxes[2, 0].set_title('Coronal - Channel 1')\naxes[2, 0].axis('off')\n\naxes[2, 1].imshow(np.rot90(X[sample, :, :, layer, 1]), cmap='gray')\naxes[2, 1].set_title('Coronal - Channel 2')\naxes[2, 1].axis('off')\n\naxes[2, 2].imshow(np.rot90(X[sample, :, :, layer, 2]), cmap='gray')\naxes[2, 2].set_title('Coronal - Channel 3')\naxes[2, 2].axis('off')\n\naxes[2, 3].imshow(np.rot90(yhat[:, :, layer]))\naxes[2, 3].set_title('Coronal - Segmentation Mask')\naxes[2, 3].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:01.330940Z","iopub.execute_input":"2024-04-28T14:29:01.331201Z","iopub.status.idle":"2024-04-28T14:29:02.829873Z","shell.execute_reply.started":"2024-04-28T14:29:01.331178Z","shell.execute_reply":"2024-04-28T14:29:02.828948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#original image\nimg_path=Train[0]\npath=os.path.join(TRAIN_DATASET_PATH,img_path)\np=os.listdir(path)\nt1 = [i for i, s in enumerate(p) if 't1.nii' in s]\nt2 = [i for i, s in enumerate(p) if 't2.nii' in s]\nt1ce = [i for i, s in enumerate(p) if 't1ce.nii' in s]\nseg = [i for i, s in enumerate(p) if 'seg.nii' in s]\nflair = [i for i, s in enumerate(p) if 'flair.nii' in s]\ntest_image_flair=nib.load(os.path.join(path,p[flair[0]])).get_fdata()\ntest_image_t1=nib.load(os.path.join(path,p[t1[0]])).get_fdata()\ntest_image_t1ce=nib.load(os.path.join(path,p[t1ce[0]])).get_fdata()\n#test_image_t2=nib.load(os.path.join(path,p[t2[0]])).get_fdata()\ntest_seg=nib.load(os.path.join(path,p[seg[0]])).get_fdata()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:02.831092Z","iopub.execute_input":"2024-04-28T14:29:02.831377Z","iopub.status.idle":"2024-04-28T14:29:03.703546Z","shell.execute_reply.started":"2024-04-28T14:29:02.831351Z","shell.execute_reply":"2024-04-28T14:29:03.702495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer=80\nplt.subplot(2,2,1)\nplt.imshow(np.rot90(test_image_t1[layer,:,:]),cmap='gray')\nplt.axis('off')\nplt.subplot(2,2,2)\nplt.imshow(np.rot90(test_image_flair[layer,:,:]),cmap='gray')\nplt.axis('off')\nplt.subplot(2,2,3)\nplt.imshow(np.rot90(test_image_t1ce[layer,:,:]),cmap='gray')\nplt.axis('off')\nplt.subplot(2,2,4)\nplt.imshow(np.rot90(test_seg[layer,:,:]))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:03.705107Z","iopub.execute_input":"2024-04-28T14:29:03.705883Z","iopub.status.idle":"2024-04-28T14:29:03.877214Z","shell.execute_reply.started":"2024-04-28T14:29:03.705843Z","shell.execute_reply":"2024-04-28T14:29:03.876559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming test_image_t1, test_image_flair, test_image_t1ce, and test_seg are your image arrays\n\nlayer = 80\n\n# Axial View\nplt.subplot(3, 4, 1)\nplt.imshow(np.rot90(test_image_t1[layer, :, :]), cmap='gray')\nplt.title('T1 Axial')\nplt.axis('off')\n\nplt.subplot(3, 4, 2)\nplt.imshow(np.rot90(test_image_flair[layer, :, :]), cmap='gray')\nplt.title('FLAIR Axial')\nplt.axis('off')\n\nplt.subplot(3, 4, 3)\nplt.imshow(np.rot90(test_image_t1ce[layer, :, :]), cmap='gray')\nplt.title('T1CE Axial')\nplt.axis('off')\n\nplt.subplot(3, 4, 4)\nplt.imshow(np.rot90(test_seg[layer, :, :]))\nplt.title('Segmentation Axial')\nplt.axis('off')\n\n# Sagittal View\nplt.subplot(3, 4, 5)\nplt.imshow(np.rot90(test_image_t1[:, layer, :]), cmap='gray')\nplt.title('T1 Sagittal')\nplt.axis('off')\n\nplt.subplot(3, 4, 6)\nplt.imshow(np.rot90(test_image_flair[:, layer, :]), cmap='gray')\nplt.title('FLAIR Sagittal')\nplt.axis('off')\n\nplt.subplot(3, 4, 7)\nplt.imshow(np.rot90(test_image_t1ce[:, layer, :]), cmap='gray')\nplt.title('T1CE Sagittal')\nplt.axis('off')\n\nplt.subplot(3, 4, 8)\nplt.imshow(np.rot90(test_seg[:, layer, :]))\nplt.title('Segmentation Sagittal')\nplt.axis('off')\n\n# Coronal View\nplt.subplot(3, 4, 9)\nplt.imshow(np.rot90(test_image_t1ce[:, :, layer]), cmap='gray')\nplt.title('T1 Coronal')\nplt.axis('off')\n\nplt.subplot(3, 4, 10)\nplt.imshow(np.rot90(test_image_flair[:, :, layer]), cmap='gray')\nplt.title('FLAIR Coronal')\nplt.axis('off')\n\nplt.subplot(3, 4, 11)\nplt.imshow(np.rot90(test_image_t1[:, :, layer]), cmap='gray')\nplt.title('T1CE Coronal')\nplt.axis('off')\n\nplt.subplot(3, 4, 12)\nplt.imshow(np.rot90(test_seg[:, :, layer]))\nplt.title('Segmentation Coronal')\nplt.axis('off')\n\n# Adjust layout to prevent clipping\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:03.878369Z","iopub.execute_input":"2024-04-28T14:29:03.879460Z","iopub.status.idle":"2024-04-28T14:29:04.965390Z","shell.execute_reply.started":"2024-04-28T14:29:03.879412Z","shell.execute_reply":"2024-04-28T14:29:04.964496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wt=np.array([0.25,0.25,0.25,0.25])\ndiceloss=sm.losses.DiceLoss(class_weights=wt)\nfocalloss=sm.losses.CategoricalFocalLoss()\ntotalloss=diceloss+(1*focalloss)\nmetrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), sm.metrics.Precision(threshold=0.5), sm.metrics.Recall(threshold=0.5)]","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:04.966635Z","iopub.execute_input":"2024-04-28T14:29:04.966976Z","iopub.status.idle":"2024-04-28T14:29:04.973385Z","shell.execute_reply.started":"2024-04-28T14:29:04.966944Z","shell.execute_reply":"2024-04-28T14:29:04.972480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=1\nlr=0.0001\noptim=keras.optimizers.Adam(lr)\nsteps_per_epoch=len(Train)//batch_size\nval_steps_per_epoch=len(Validation)//batch_size","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:04.974555Z","iopub.execute_input":"2024-04-28T14:29:04.974845Z","iopub.status.idle":"2024-04-28T14:29:05.780375Z","shell.execute_reply.started":"2024-04-28T14:29:04.974821Z","shell.execute_reply":"2024-04-28T14:29:05.779330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=sm.Unet(backbone_name='efficientnetb0', input_shape=(128,128,128,3), classes=4, activation='softmax',encoder_weights='imagenet')\n# model.compile(optimizer=optim,loss=totalloss,metrics=metrics)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:05.781544Z","iopub.execute_input":"2024-04-28T14:29:05.781884Z","iopub.status.idle":"2024-04-28T14:29:05.786130Z","shell.execute_reply.started":"2024-04-28T14:29:05.781853Z","shell.execute_reply":"2024-04-28T14:29:05.785219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\n\ndef conv_block(x, filters, kernel_size=(3, 3, 3), activation='relu', padding='same'):\n    x = layers.Conv3D(filters, kernel_size, activation=activation, padding=padding)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv3D(filters, kernel_size, activation=activation, padding=padding)(x)\n    x = layers.BatchNormalization()(x)\n    return x\n\ndef unet_3d(input_shape=(128, 128, 128, 3), num_classes=4):\n    inputs = layers.Input(input_shape)\n\n    # Encoder\n    conv1 = conv_block(inputs, 32)\n    pool1 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n\n    conv2 = conv_block(pool1, 64)\n    pool2 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n\n    conv3 = conv_block(pool2, 128)\n    pool3 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n\n    # Middle\n    conv4 = conv_block(pool3, 256)\n\n    # Decoder\n    up5 = layers.UpSampling3D(size=(2, 2, 2))(conv4)\n    concat5 = layers.concatenate([conv3, up5], axis=-1)\n    conv5 = conv_block(concat5, 128)\n\n    up6 = layers.UpSampling3D(size=(2, 2, 2))(conv5)\n    concat6 = layers.concatenate([conv2, up6], axis=-1)\n    conv6 = conv_block(concat6, 64)\n\n    up7 = layers.UpSampling3D(size=(2, 2, 2))(conv6)\n    concat7 = layers.concatenate([conv1, up7], axis=-1)\n    conv7 = conv_block(concat7, 32)\n\n    outputs = layers.Conv3D(num_classes, (1, 1, 1), activation='softmax')(conv7)\n\n    model = Model(inputs, outputs, name='3D_UNet')\n    return model\n\n# Instantiate the model\nmodel = unet_3d(input_shape=(128, 128, 128, 3), num_classes=4)\n\n# Compile the model\nmodel.compile(optimizer=optim, loss=totalloss, metrics=metrics)\n\n# Display model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:05.787320Z","iopub.execute_input":"2024-04-28T14:29:05.787659Z","iopub.status.idle":"2024-04-28T14:29:06.299972Z","shell.execute_reply.started":"2024-04-28T14:29:05.787627Z","shell.execute_reply":"2024-04-28T14:29:06.299008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import layers\n# from tensorflow.keras.models import Model\n\n# def conv_block(x, filters, kernel_size=(3, 3, 3), activation='leakyrelu', padding='same'):\n#     if activation == 'leakyrelu':\n#         activation = layers.LeakyReLU(alpha=0.1)\n#     x = layers.Conv3D(filters, kernel_size, activation=activation, padding=padding)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters, kernel_size, activation=activation, padding=padding)(x)\n#     x = layers.BatchNormalization()(x)\n#     return x\n\n# def unet_3d(input_shape=(128, 128, 128, 3), num_classes=4):\n#     inputs = layers.Input(input_shape)\n\n#     # Encoder\n#     conv1 = conv_block(inputs, 32)\n#     pool1 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n\n#     conv2 = conv_block(pool1, 64)\n#     pool2 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n\n#     conv3 = conv_block(pool2, 128)\n#     pool3 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n\n#     conv4 = conv_block(pool3, 256)\n#     pool4 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n\n#     conv5 = conv_block(pool4, 512)\n#     pool5 = layers.MaxPooling3D(pool_size=(2, 2, 2))(conv5)\n\n#     conv6 = conv_block(pool5, 1024)\n\n#     # Decoder\n#     up7 = layers.UpSampling3D(size=(2, 2, 2))(conv6)\n#     concat7 = layers.concatenate([conv5, up7], axis=-1)\n#     conv7 = conv_block(concat7, 512)\n\n#     up8 = layers.UpSampling3D(size=(2, 2, 2))(conv7)\n#     concat8 = layers.concatenate([conv4, up8], axis=-1)\n#     conv8 = conv_block(concat8, 256)\n\n#     up9 = layers.UpSampling3D(size=(2, 2, 2))(conv8)\n#     concat9 = layers.concatenate([conv3, up9], axis=-1)\n#     conv9 = conv_block(concat9, 128)\n\n#     up10 = layers.UpSampling3D(size=(2, 2, 2))(conv9)\n#     concat10 = layers.concatenate([conv2, up10], axis=-1)\n#     conv10 = conv_block(concat10, 64)\n\n#     up11 = layers.UpSampling3D(size=(2, 2, 2))(conv10)\n#     concat11 = layers.concatenate([conv1, up11], axis=-1)\n#     conv11 = conv_block(concat11, 32)\n\n#     # Additional Convolutional Blocks\n#     conv12 = conv_block(conv11, 16)\n#     conv13 = conv_block(conv12, 8)\n\n#     outputs = layers.Conv3D(num_classes, (1, 1, 1), activation='softmax')(conv13)\n\n#     model = Model(inputs, outputs, name='3D_UNet')\n#     return model\n\n# # Instantiate the model\n# model = unet_3d(input_shape=(128, 128, 128, 3), num_classes=4)\n\n# # Compile the model\n# model.compile(optimizer=optim, loss=totalloss, metrics=metrics)\n\n# # Display model summary\n# model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:06.301723Z","iopub.execute_input":"2024-04-28T14:29:06.302088Z","iopub.status.idle":"2024-04-28T14:29:06.308488Z","shell.execute_reply.started":"2024-04-28T14:29:06.302054Z","shell.execute_reply":"2024-04-28T14:29:06.307492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True,expand_nested = True,dpi = 80)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:06.309655Z","iopub.execute_input":"2024-04-28T14:29:06.309931Z","iopub.status.idle":"2024-04-28T14:29:06.931315Z","shell.execute_reply.started":"2024-04-28T14:29:06.309907Z","shell.execute_reply":"2024-04-28T14:29:06.930356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualkeras.layered_view(model, legend=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:06.932552Z","iopub.execute_input":"2024-04-28T14:29:06.932825Z","iopub.status.idle":"2024-04-28T14:29:07.928662Z","shell.execute_reply.started":"2024-04-28T14:29:06.932801Z","shell.execute_reply":"2024-04-28T14:29:07.927794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stop = EarlyStopping(monitor='val_loss', mode = 'min', patience=5, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:07.936814Z","iopub.execute_input":"2024-04-28T14:29:07.937191Z","iopub.status.idle":"2024-04-28T14:29:07.942717Z","shell.execute_reply.started":"2024-04-28T14:29:07.937155Z","shell.execute_reply":"2024-04-28T14:29:07.941788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(Train_datagen,epochs=50,validation_data=Val_datagen,verbose=1,steps_per_epoch=steps_per_epoch,validation_steps=val_steps_per_epoch, callbacks = [early_stop])\n# model.save('my_mdl.keras')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:29:07.943873Z","iopub.execute_input":"2024-04-28T14:29:07.944153Z","iopub.status.idle":"2024-04-28T17:16:46.053344Z","shell.execute_reply.started":"2024-04-28T14:29:07.944129Z","shell.execute_reply":"2024-04-28T17:16:46.052322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Train_accuracy')\nplt.plot(history.history['accuracy'])\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:46.122888Z","iopub.execute_input":"2024-04-28T17:16:46.123159Z","iopub.status.idle":"2024-04-28T17:16:46.331778Z","shell.execute_reply.started":"2024-04-28T17:16:46.123136Z","shell.execute_reply":"2024-04-28T17:16:46.330871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Validation_accuracy')\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:46.332699Z","iopub.execute_input":"2024-04-28T17:16:46.332972Z","iopub.status.idle":"2024-04-28T17:16:46.597532Z","shell.execute_reply.started":"2024-04-28T17:16:46.332947Z","shell.execute_reply":"2024-04-28T17:16:46.596567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Train_IOU_score')\nplt.plot(history.history['iou_score'])\nplt.xlabel('Epochs')\nplt.ylabel('IOU Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:46.598818Z","iopub.execute_input":"2024-04-28T17:16:46.599568Z","iopub.status.idle":"2024-04-28T17:16:46.805936Z","shell.execute_reply.started":"2024-04-28T17:16:46.599533Z","shell.execute_reply":"2024-04-28T17:16:46.804959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Validation_IOU_score')\nplt.plot(history.history['val_iou_score'])\nplt.xlabel('Epochs')\nplt.ylabel('IOU Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:46.807310Z","iopub.execute_input":"2024-04-28T17:16:46.807726Z","iopub.status.idle":"2024-04-28T17:16:47.119007Z","shell.execute_reply.started":"2024-04-28T17:16:46.807688Z","shell.execute_reply":"2024-04-28T17:16:47.118118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'],label='Train_loss')\nplt.plot(history.history['val_loss'],label='Val_loss')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:47.120755Z","iopub.execute_input":"2024-04-28T17:16:47.121132Z","iopub.status.idle":"2024-04-28T17:16:47.420940Z","shell.execute_reply.started":"2024-04-28T17:16:47.121097Z","shell.execute_reply":"2024-04-28T17:16:47.420029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y=Test_datagen.__getitem__(10)#fetching the first batch","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:47.422255Z","iopub.execute_input":"2024-04-28T17:16:47.422638Z","iopub.status.idle":"2024-04-28T17:16:49.135608Z","shell.execute_reply.started":"2024-04-28T17:16:47.422603Z","shell.execute_reply":"2024-04-28T17:16:49.134719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = y\ny_pred=model.predict(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:49.136954Z","iopub.execute_input":"2024-04-28T17:16:49.137261Z","iopub.status.idle":"2024-04-28T17:16:50.136806Z","shell.execute_reply.started":"2024-04-28T17:16:49.137234Z","shell.execute_reply":"2024-04-28T17:16:50.135921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=0#checking the 2nd sample of first batch\nyhat=y[sample]\nyhat=np.argmax(yhat,axis=-1)\nlayer=120\nplt.subplot(2,2,1)\nplt.imshow(np.rot90(X[sample,layer,:,:,0]),cmap='gray')\nplt.axis('off')\nplt.subplot(2,2,2)\nplt.imshow(np.rot90(X[sample,layer,:,:,1]),cmap='gray')\nplt.axis('off')\nplt.subplot(2,2,3)\nplt.imshow(np.rot90(X[sample,layer,:,:,2]))\nplt.axis('off')\nplt.subplot(2,2,4)\nplt.imshow(np.rot90(yhat[layer,:,:]))\nplt.axis('off')\nplt.show()\n\nyhat=np.argmax(y_pred[0],axis=-1)\nplt.imshow(np.rot90(yhat[layer,:,:]))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:50.138256Z","iopub.execute_input":"2024-04-28T17:16:50.138896Z","iopub.status.idle":"2024-04-28T17:16:50.401017Z","shell.execute_reply.started":"2024-04-28T17:16:50.138859Z","shell.execute_reply":"2024-04-28T17:16:50.399249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming Test_datagen is your test data generator\n# and model is your segmentation model\n\n# Initialize empty lists to store true and predicted segmentation masks\ny_true_list = []\ny_pred_list = []\n\n# Loop through the test data generator to predict on the full test set\nfor i in range(len(Test_datagen)):\n    X, y = Test_datagen.__getitem__(i)\n    y_true_list.append(y)\n    y_pred = model.predict(X)\n    y_pred_list.append(y_pred)\n\n# Concatenate the list of true and predicted masks into numpy arrays\ny_true = np.concatenate(y_true_list, axis=0)\ny_pred = np.concatenate(y_pred_list, axis=0)\n\n# Now you have the full true and predicted segmentation masks\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:16:50.402212Z","iopub.execute_input":"2024-04-28T17:16:50.402640Z","iopub.status.idle":"2024-04-28T17:20:27.141552Z","shell.execute_reply.started":"2024-04-28T17:16:50.402614Z","shell.execute_reply":"2024-04-28T17:20:27.140486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_true.shape)\nprint(y_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:20:27.144332Z","iopub.execute_input":"2024-04-28T17:20:27.145274Z","iopub.status.idle":"2024-04-28T17:20:27.150681Z","shell.execute_reply.started":"2024-04-28T17:20:27.145244Z","shell.execute_reply":"2024-04-28T17:20:27.149847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# s_y_true = y_true[:20]\n# s_y_pred = y_pred[:20]","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:20:27.151703Z","iopub.execute_input":"2024-04-28T17:20:27.151979Z","iopub.status.idle":"2024-04-28T17:20:27.175112Z","shell.execute_reply.started":"2024-04-28T17:20:27.151949Z","shell.execute_reply":"2024-04-28T17:20:27.174210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(s_y_true.shape)\n# print(s_y_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:20:27.176198Z","iopub.execute_input":"2024-04-28T17:20:27.176480Z","iopub.status.idle":"2024-04-28T17:20:27.185318Z","shell.execute_reply.started":"2024-04-28T17:20:27.176444Z","shell.execute_reply":"2024-04-28T17:20:27.184479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef dice_coefficient(y_true, y_pred):\n    dice_scores = {}\n    for t in range(y_true.shape[-1]):\n        intersection = np.sum(y_true[..., t] * y_pred[..., t], axis=(1,2,3))\n        union = np.sum(y_true[..., t], axis=(1,2,3)) + np.sum(y_pred[..., t], axis=(1,2,3))\n        dice_scores[f\"type {t+1}\"] = np.mean((2. * intersection + 1e-6) / (union + 1e-6)) * 100\n    return dice_scores\n\ndef intersection_over_union(y_true, y_pred):\n    iou_scores = {}\n    for t in range(y_true.shape[-1]):\n        intersection = np.sum(y_true[..., t] * y_pred[..., t], axis=(1,2,3))\n        union = np.sum(y_true[..., t], axis=(1,2,3)) + np.sum(y_pred[..., t], axis=(1,2,3)) - intersection\n        iou_scores[f\"type {t+1}\"] = np.mean((intersection + 1e-6) / (union + 1e-6)) * 100\n    return iou_scores\n\ndef precision(y_true, y_pred):\n    precision_scores = {}\n    for t in range(y_true.shape[-1]):\n        true_positives = np.sum(y_true[..., t] * y_pred[..., t], axis=(1,2,3))\n        all_positives = np.sum(y_pred[..., t], axis=(1,2,3))\n        precision_scores[f\"type {t+1}\"] = np.mean((true_positives + 1e-6) / (all_positives + 1e-6)) * 100\n    return precision_scores\n\ndef recall(y_true, y_pred):\n    recall_scores = {}\n    for t in range(y_true.shape[-1]):\n        true_positives = np.sum(y_true[..., t] * y_pred[..., t], axis=(1,2,3))\n        all_true = np.sum(y_true[..., t], axis=(1,2,3))\n        recall_scores[f\"type {t+1}\"] = np.mean((true_positives + 1e-6) / (all_true + 1e-6)) * 100\n    return recall_scores\n\ndef accuracy(y_true, y_pred):\n    accuracy_scores = {}\n    for t in range(y_true.shape[-1]):\n        correct_predictions = np.sum((y_true[..., t] == 1) & (y_pred[..., t] == 1), axis=(1,2,3))\n        total_predictions = np.sum(y_true[..., t] == 1, axis=(1,2,3))\n        accuracy_scores[f\"type {t+1}\"] = np.mean((correct_predictions + 1e-6) / (total_predictions + 1e-6)) * 100\n    return accuracy_scores\n\ndef f1_score(y_true, y_pred):\n    f1_scores = {}\n    for t in range(y_true.shape[-1]):\n        precisions = precision(y_true, y_pred)\n        recalls = recall(y_true, y_pred)\n        f1_scores[f\"type {t+1}\"] = 2 * (precisions[f\"type {t+1}\"] * recalls[f\"type {t+1}\"]) / (precisions[f\"type {t+1}\"] + recalls[f\"type {t+1}\"] + 1e-6)\n    return f1_scores\n\ndef compute_metrics(y_true, y_pred):\n    dice = dice_coefficient(y_true, y_pred)\n    iou = intersection_over_union(y_true, y_pred)\n    prec = precision(y_true, y_pred)\n    rec = recall(y_true, y_pred)\n    acc = accuracy(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    return dice, iou, prec, rec, acc, f1\n\n# Example usage:\n# Assuming y_true and y_pred are 5D arrays representing ground truth and predicted segmentations respectively\n# Shape of y_true and y_pred should be (total_images, depth, height, width, types)\n\ndice_scores, iou_scores, prec_scores, rec_scores, acc_scores, f1_scores = compute_metrics(y_true, y_pred)\n\nprint(\"Dice:\")\nfor key, value in dice_scores.items():\n    print(f\"{key}: {value:.2f}\")\n\nprint(\"\\nIoU:\")\nfor key, value in iou_scores.items():\n    print(f\"{key}: {value:.2f}\")\n\nprint(\"\\nPrecision:\")\nfor key, value in prec_scores.items():\n    print(f\"{key}: {value:.2f}\")\n\nprint(\"\\nRecall:\")\nfor key, value in rec_scores.items():\n    print(f\"{key}: {value:.2f}\")\n\nprint(\"\\nAccuracy:\")\nfor key, value in acc_scores.items():\n    print(f\"{key}: {value:.2f}\")\n\nprint(\"\\nF1 Score:\")\nfor key, value in f1_scores.items():\n    print(f\"{key}: {value:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:20:27.186496Z","iopub.execute_input":"2024-04-28T17:20:27.186816Z","iopub.status.idle":"2024-04-28T17:22:06.495238Z","shell.execute_reply.started":"2024-04-28T17:20:27.186785Z","shell.execute_reply":"2024-04-28T17:22:06.494283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# import os\n# os.chdir(r'/kaggle/working/')\n# FileLink(r'/kaggle/working/UNet_3D_best_model_round3_128.keras')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:22:06.497729Z","iopub.execute_input":"2024-04-28T17:22:06.498384Z","iopub.status.idle":"2024-04-28T17:22:06.503489Z","shell.execute_reply.started":"2024-04-28T17:22:06.498347Z","shell.execute_reply":"2024-04-28T17:22:06.502591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save('saved_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:22:06.504777Z","iopub.execute_input":"2024-04-28T17:22:06.505036Z","iopub.status.idle":"2024-04-28T17:22:06.519087Z","shell.execute_reply.started":"2024-04-28T17:22:06.505012Z","shell.execute_reply":"2024-04-28T17:22:06.518203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('saved_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:22:06.520185Z","iopub.execute_input":"2024-04-28T17:22:06.520594Z","iopub.status.idle":"2024-04-28T17:22:06.653479Z","shell.execute_reply.started":"2024-04-28T17:22:06.520570Z","shell.execute_reply":"2024-04-28T17:22:06.652484Z"},"trusted":true},"execution_count":null,"outputs":[]}]}